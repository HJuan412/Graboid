#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Nov 30 11:06:10 2021

@author: hernan
"""

import sys
sys.path.append('preprocess')
sys.path.append('classif')
#%% libraries
import concurrent.futures
import logging
import numba as nb
import numpy as np
import pandas as pd
import pickle
import time
from classification import classification
from classification import cost_matrix
import os
from preprocess import feature_selection as fsele
# from preprocess import taxon_study as tstud
from preprocess import windows

#%% set
logger = logging.getLogger('Graboid.calibrator')
logger.setLevel(logging.DEBUG)

#%% functions
def make_dirs(base_dir):
    os.makedirs(f'{base_dir}/data', exist_ok=bool)
    os.makedirs(f'{base_dir}/warnings', exist_ok=bool)

def get_metrics(confusion, taxons):
    # get calibration metrics for a given confusion matrix
    # confusion: confusion matrix
    # taxons: taxon index (generated by build_confusion)

    results = []
    for idx, tax in enumerate(taxons):
        # get true/false positives/negatives for each taxon
        tp = confusion[idx, idx]
        tn = np.delete(np.delete(confusion, idx, axis=0), idx, axis=1).sum()
        fp = np.delete(confusion[idx], idx).sum()
        fn = np.delete(confusion[:, idx], idx).sum()
        
        # calculate metrics (accuracy, precision, recall and F1)
        acc = (tp + tn) / (tp + tn + fp + fn)
        prc = 0
        rec = 0
        f1 = 0
        if tp > 0:
            # if there are no positive values, prec, rec and f1 are 0 by default
            prc = tp / (tp + fp)
            rec = tp / (tp + fn)
            f1 = (2 * prc * rec)/(prc + rec)
        
        results.append([tax, acc, prc, rec, f1])
    return results

def loo_generator(nrecs):
    # generates the indexes for the training dataset and the testing instance in leave-one-out calibration
    record_idxs = np.arange(nrecs)
    for idx in record_idxs:
        train_idx = np.delete(record_idxs, idx)
        test_idx = idx
        yield train_idx, test_idx

@nb.njit
def build_confusion(pred, real):
    # build the confusion matrix (for a given RANK)
    # pred: array of PREDICTED taxon for each training sample
    # pred: array of REAL taxon for each training sample

    # list & count taxes
    uniq_taxes = np.unique(real)
    n_taxes = len(uniq_taxes)
    
    confusion = np.zeros((n_taxes, n_taxes), dtype=np.int32)
    # rows: REAL taxons
    # columns: PREDICTED taxons
    
    for idx_0, tax_0 in enumerate(uniq_taxes):
        tax_pred = pred[real == tax_0]
        for idx_1, tax_1 in enumerate(uniq_taxes):
            pred_as = len(tax_pred[tax_pred == tax_1])
            confusion[idx_0, idx_1] = pred_as
    return confusion, uniq_taxes
    
def build_cal_tab(pred_tax, real_tax, n_ranks=6):
    # build the calibration table from the given results
    # n_ranks 6 by default (phylum, class, order, family, genus, species), could be modifyed to include less/more (should be done automatically)
    
    results = []
    # remove first(query name) and last (K) columns from predicted taxons
    pred_cropped = pred_tax[:,1:-1].T
    real_mat = real_tax.to_numpy().T
    ranks = real_tax.columns
    
    for rank, pred, real in zip(ranks, pred_cropped, real_mat):
        # build a confusion matrix and get results from there, update results table
        rank_confusion, taxons = build_confusion(pred, real)
        rank_metrics = get_metrics(rank_confusion, taxons)
        rank_metrics = np.insert(rank_metrics, 0, rank, axis=1)
        results.append(rank_metrics)
    return np.concatenate(results)

    for rank in np.arange(n_ranks):
        # build a confusion matrix and get results from there, update results table
        rank_confusion, taxons = build_confusion(pred_cropped[:,rank], real_tax.iloc[:,rank].to_numpy())
        rank_metrics = get_metrics(rank_confusion, taxons)
        rank_metrics = np.insert(rank_metrics, 0, rank, axis=1)
        results.append(rank_metrics)
    return np.concatenate(results)

#%% classes
class Calibrator:
    def __init__(self, out_dir, warn_dir):
        self.out_dir = out_dir
        self.warn_dir = warn_dir
        
        self.selector = fsele.Selector(out_dir)
        self.loader = None
        self.set_row_thresh()
        self.set_col_thresh()
        self.set_min_seqs()
        self.set_rank()
        self.set_dist_mat('id')
        self.report = None
    
    def set_database(self, mat_file, acc_file, tax_file, order_file):
        self.loader = windows.WindowLoader('Graboid.calibrator.windowloader')
        self.loader.set_files(mat_file, acc_file, tax_file)
        self.selector.load_order_mat(order_file)
    
    def set_row_thresh(self, thresh=0.2):
        self.row_thresh = thresh
        
    def set_col_thresh(self, thresh=0.2):
        self.col_thresh = thresh
    
    def set_min_seqs(self, min_seqs=10):
        self.min_seqs = min_seqs
    
    def set_rank(self, rank='genus'):
        self.rank = rank
    
    def set_dist_mat(self, mat_code):
        matrix = cost_matrix.get_matrix(mat_code)
        if matrix is None:
            print('Could not set distance matrix, invalid matrix code')
            return
        self.cost_mat = matrix
    
    def check_ready(self):
        # check that the calibration is ready to go
        missing = []
        try:
            self.cost_mat
        except AttributeError:
            missing.append('distance matrix')
        try:
            self.loader.matrix
            self.loader.bounds
            self.loader.dims
        except AttributeError:
            missing.append('alignment matrix')
        try:
            self.loader.accdist
        except AttributeError:
            missing.append('accession list')
        try:
            self.loader.tax_tab            
        except AttributeError:
            missing.append('taxonomy table')
        try:
            self.order_file
        except AttributeError:
            missing.append('order file')
        ready = len(missing) > 0
        return ready, missing
    
    def grid_search_OLD(self, w_size, w_step, max_k, step_k, max_n, step_n, min_k=1, min_n=5, filename=None):
        if self.loader is None:
            return
        # prepare out files
        if filename is None:
            filename = time.strftime("report_%d%m%Y-%H%M%S")
        self.out_file = f'{self.out_dir}/{filename}.csv'
        self.meta_file = f'{self.out_dir}/{filename}.meta'
        pd.DataFrame(columns=['Taxon',
                              'Accuracy',
                              'Precision',
                              'Recall',
                              'F1_score',
                              'w_start',
                              'w_end',
                              'rank',
                              'n_sites',
                              'K',
                              'mode']).to_csv(self.out_file, index=False)
        # set calibration parameters
        # set coordinate ranges for the sliding window
        max_pos = self.loader.dims[1]
        start_range = np.arange(0, max_pos - w_size, w_step)
        if start_range[-1] < max_pos - w_size:
            # add a tail window, if needed, to cover the entire sequence
            np.append(start_range, max_pos - w_size)
        end_range = start_range + w_size
        w_coords = np.array([start_range, end_range]).T
        
        # k & n ranges
        k_range = np.arange(min_k, max_k, step_k)
        n_range = np.arange(min_n, max_n, step_n)
        
        # begin calibration
        for idx, (start, end) in enumerate(w_coords):
            print(f'Window {start} - {end} ({idx + 1} of {len(w_coords)})')
            results = []
            # extract window and select atributes
            window = self.loader.get_window(start, end, self.row_thresh, self.col_thresh)
            if len(window.eff_mat) == 0:
                continue
            n_seqs = window.eff_mat.shape[0]
            quintile = int(n_seqs / 5)

            if n_seqs < self.min_seqs:
                # not enough sequences passed the filter, skip iteration
                print(f'Window {start} - {end}. Not enoug sequences to perform calibration ({n_seqs}, min = {self.min_seqs}), skipping')
                continue
            
            n_sites = self.selector.get_sites(n_range, start, end, self.rank)
            y = window.eff_tax
            prev_distances = np.zeros((n_seqs, n_seqs-1), dtype = np.float32)
            reports1 = []
            for n, sites in n_sites.items():
                print(f'\t{n} sites')
                # post collapse, generates the final data matrix and taxonomy table
                x = window.eff_mat[:, sites]
                
                # loo_result tables are plain lists, later to be converted into np.arrays
                # these store the classifications for each query
                
                # iterate trought the data
                idx_gen = loo_generator(x.shape[0])
                
                reports0 = []
                # outer loop
                for train_idx, test_idx in idx_gen:
                    if test_idx % quintile == 0:
                        print(f'\t\tCalibrated {test_idx} of {n_seqs} ({(test_idx/n_seqs) * 100:.2f}%)')
                    
                    # build the datasets
                    query = x[[test_idx]]
                    train_data = x[train_idx]
                    train_tax = y.iloc[train_idx]
                    query_dists = prev_distances[test_idx]
                    q_report, new_dists = classification.classify(np.reshape(query, (1,-1)), train_data, self.cost_mat, train_tax, k_range, modes = 'mwd', prev_dists = query_dists, get_winners = True)
                    # q_report columns: idx, rk, tax, count, _k, average_dists, std_dists, total_support, median_support
                    prev_distances[[test_idx]] += new_dists
                    reports0.append(q_report)
                reports0 = pd.concat(reports0)
                reports0.insert(2, 'w_start', start)
                reports0.insert(3, 'w_end', end)
                reports0.insert(4, 'n_sites', n)
                reports1.append(reports0)
                
            report = pd.concat(reports1)
            report.sort_values(['w_start', 'n_sites', '_k'], inplace=True, ignore_index=True)
            
            # get metrics for each tank/tax in each mode,K,n_sites combination
            for (w_start, n, k, mode), subtab0 in report.groupby(['w_start', 'n_sites', '_k', 'mode']):
                w_end = subtab0.w_end.iloc[0]
                for (rk, tax), subtab1 in subtab0.groupby(['rk', 'tax']):
                    pred = subtab1.tax.values
                    real = y.loc[subtab1.idx].iloc[:, int(rk)].values
                    confusion, taxons = build_confusion(pred, real)
                    metrics = get_metrics(confusion, taxons)
                    metrics_report = pd.DataFrame(metrics, columns=['Taxon', 'Accuracy', 'Precision', 'Recall', 'F1_score'])
                    metrics_report['w_start'] = w_start
                    metrics_report['w_end'] = w_end
                    metrics_report['rank'] = rk
                    # metrics_report['taxon'] = tax
                    metrics_report['n_sites'] = n
                    metrics_report['K'] = k
                    metrics_report['mode'] = mode
                    
                    results.append(metrics_report)
            results = pd.concat(results)
            # translate numeric codes
            numcode_dict = {idx:rank for idx, rank in enumerate(self.loader.tax_tab.columns)}
            results['rank'].replace(numcode_dict, inplace=True)
            results.to_csv(self.out_file, header=False, index=False, mode='a')
            
        self.meta = {'k':k_range,
                     'n':n_range,
                     'w_size':w_size,
                     'w_step':w_step}
        with open(self.meta_file, 'wb') as meta_handle:
            pickle.dump(self.meta, meta_handle)
    
    def grid_search(self, w_size, w_step, max_k, step_k, max_n, step_n, min_k=1, min_n=5, filename=None, threads=1, keep_classif=False):
        if self.loader is None:
            return
        
        # prepare out files
        if filename is None:
            filename = time.strftime("report_%d%m%Y-%H%M%S")
        self.out_file = f'{self.out_dir}/{filename}.csv'
        self.classif_file = f'{self.out_dir}/classif_{filename}.csv'
        self.meta_file = f'{self.out_dir}/{filename}.meta'
        header = True # toggle this when a report file hasn't been generated yet, use to control inclusion of header col
        
        # set calibration parameters
        # set coordinate ranges for the sliding window
        max_pos = self.loader.dims[1]
        start_range = np.arange(0, max_pos - w_size, w_step)
        if start_range[-1] < max_pos - w_size:
            # add a tail window, if needed, to cover the entire sequence
            np.append(start_range, max_pos - w_size)
        end_range = start_range + w_size
        w_coords = np.array([start_range, end_range]).T
        
        # k & n ranges
        k_range = np.arange(min_k, max_k, step_k)
        n_range = np.arange(min_n, max_n, step_n)
        
        # begin calibration
        for idx, (start, end) in enumerate(w_coords):
            t0 = time.time()
            print(f'Window {start} - {end} ({idx + 1} of {len(w_coords)})')
            # extract window and select atributes
            window = self.loader.get_window(start, end, self.row_thresh, self.col_thresh)
            if len(window.eff_mat) == 0:
                continue
            n_seqs = window.eff_mat.shape[0]
            if n_seqs < self.min_seqs:
                # not enough sequences passed the filter, skip iteration
                print(f'Window {start} - {end}. Not enoug sequences to perform calibration ({n_seqs}, min = {self.min_seqs}), skipping')
                continue
            # n_sites = self.selector.get_sites(n_range, start, end, self.rank)
            n_sites = self.selector.get_sites(n_range, self.rank, window.cols)
            y = window.eff_tax
            # distance container, 3d array, paired distance matrix for every value of n
            dist_mat = np.zeros((n_seqs, n_seqs, len(n_range)), dtype=np.float32)
            # get paired distances
            t1 = time.time()
            logger.debug(f'prep time {t1 - t0}')
            for idx_0 in np.arange(n_seqs - 1):
                qry_seq = window.eff_mat[[idx_0]]
                idx_1 = idx_0 + 1
                ref_seqs = window.eff_mat[idx_1:]
                # persistent distance array, updates with each value of n
                dists = np.zeros((1, ref_seqs.shape[0]), dtype=np.float32)
                for n_idx, sites in enumerate(n_sites.values()):
                    sub_qry = qry_seq[:, sites]
                    sub_ref = ref_seqs[:, sites]
                    dists += classification.get_dists(sub_qry, sub_ref, self.cost_mat).reshape(1, -1)
                    dist_mat[idx_0, idx_1:, n_idx] = dists
                    dist_mat[idx_1:, idx_0, n_idx] = dists
            t2 = time.time()
            logger.debug(f'dist calculation {t2 - t1}')
            # get ordered_neighbours and sorted distances (excluding first columns, which corresponds to the diagonal)
            neighbours = np.argsort(dist_mat, axis=1)[:,1:,:]
            ordered_dists = [dist_mat[np.tile(np.arange(n_seqs), (n_seqs-1, 1)).T, neighbours[...,n], n] for n in range(neighbours.shape[2])]
            
            guide = [(n, mode, classif) for mode, classif in classification.classif_funcs_nb.items() for n in range(len(n_range))]
            classif_report = []
            # use multiprocessing to speed up classification
            t3 = time.time()
            with concurrent.futures.ProcessPoolExecutor(max_workers=threads) as executor:
                # since we're using numba functions, y must be cast as a numpy array
                future_classifs = {executor.submit(classifier, neighbours[...,n], ordered_dists[n], y.to_numpy(), k_range):(mode,n) for (n, mode, classifier) in guide}
                for future in concurrent.futures.as_completed(future_classifs):
                    (pre_classif, columns) = future.result()
                    (mode, n) = future_classifs[future]
                    classif = classification.get_classif(pre_classif, classification.classif_modes[mode])
                    mode_report = pd.DataFrame(classif, columns=columns)
                    mode_report['mode'] = mode
                    mode_report['n'] = n_range[n]
                    classif_report.append(mode_report)
            classif_report = pd.concat(classif_report)
            t4 = time.time()
            logger.debug(f'classification {t4 - t3}')
            # store intermediate classification results (if enabled)
            if keep_classif:
                classif_report['w_start'] = start
                classif_report['w_end'] = end
                classif_report.to_csv(self.classif_file, header=header, index=False, mode='a')
            # get classification metrics
            t5 = time.time()
            for (k, n, mode), subtab in classif_report.groupby(['_k', 'n', 'mode']):
                for rk, rk_subtab in subtab.groupby('rk'):
                    pred = rk_subtab.tax.values
                    real = y.loc[rk_subtab.idx.values].iloc[:,int(rk)].values
                    confusion, taxons = build_confusion(pred, real)
                    metrics = get_metrics(confusion, taxons)
                    metrics_report = pd.DataFrame(metrics, columns=['Taxon', 'Accuracy', 'Precision', 'Recall', 'F1_score'])
                    metrics_report['w_start'] = start
                    metrics_report['w_end'] = end
                    metrics_report['rank'] = rk
                    metrics_report['n_sites'] = n
                    metrics_report['K'] = k
                    metrics_report['mode'] = classification.classif_longnames[mode]
                    
                    metrics_report.to_csv(self.out_file, header=header, index=False, mode='a')
                    header = False
            t6 = time.time()
            logger.debug(f'metric calculation {t6 - t5}')
            
            self.meta = {'k':k_range,
                         'n':n_range,
                         'w_size':w_size,
                         'w_step':w_step}
            with open(self.meta_file, 'wb') as meta_handle:
                pickle.dump(self.meta, meta_handle)

    def save_report(self, filename=None):
        if filename is None:
            filename = time.strftime("report_%d%m%Y-%H%M%S")
        self.out_file = f'{self.out_dir}/{filename}.csv'
        self.meta_file = f'{self.out_dir}/{filename}.meta'
        if not self.report is None:
            self.report.to_csv(self.out_file)
            with open(self.meta_file, 'wb') as meta_handle:
                pickle.dump(self.meta, meta_handle)
            logger.info(f'Calibration report saved to {self.out_file}')