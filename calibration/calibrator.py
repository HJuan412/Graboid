#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Nov 30 11:06:10 2021

@author: hernan
"""

import sys
sys.path.append('preprocess')
sys.path.append('classif')
#%% libraries
import logging
import numpy as np
import pandas as pd
import pickle
import time
from classification import classification
from classification import cost_matrix
import os
from preprocess import feature_selection as fsele
from preprocess import preproc
from preprocess import taxon_study as tstud
from preprocess import windows

#%% set
logger = logging.getLogger('Graboid.calibrator')
logger.setLevel(logging.DEBUG)

#%% functions
def make_dirs(base_dir):
    os.makedirs(f'{base_dir}/data', exist_ok=bool)
    os.makedirs(f'{base_dir}/warnings', exist_ok=bool)

def get_metrics(confusion, taxons):
    # get calibration metrics for a given confusion matrix
    # confusion: confusion matrix
    # taxons: taxon index (generated by build_confusion)

    results = []
    for idx, tax in enumerate(taxons):
        # get true/false positives/negatives for each taxon
        tp = confusion[idx, idx]
        tn = np.delete(np.delete(confusion, idx, axis=0), idx, axis=1).sum()
        fp = np.delete(confusion[idx], idx).sum()
        fn = np.delete(confusion[:, idx], idx).sum()
        
        # calculate metrics (accuracy, precision, recall and F1)
        acc = (tp + tn) / (tp + tn + fp + fn)
        prc = 0
        rec = 0
        f1 = 0
        if tp > 0:
            # if there are no positive values, prec, rec and f1 are 0 by default
            prc = tp / (tp + fp)
            rec = tp / (tp + fn)
            f1 = (2 * prc * rec)/(prc + rec)
        
        results.append([tax, acc, prc, rec, f1])
    return results

def loo_generator(nrecs):
    # generates the indexes for the training dataset and the testing instance in leave-one-out calibration
    record_idxs = np.arange(nrecs)
    for idx in record_idxs:
        train_idx = np.delete(record_idxs, idx)
        test_idx = idx
        yield train_idx, test_idx

def build_confusion(pred, real):
    # build the confusion matrix (for a given RANK)
    # pred: array of PREDICTED taxon for each training sample
    # pred: array of REAL taxon for each training sample

    confusion = []
    # rows: REAL taxons
    # columns: PREDICTED taxons

    # list & count taxes
    uniq_taxes = np.unique(real)
    n_taxes = len(uniq_taxes)
    
    for tax in uniq_taxes:
        pred_counts = np.zeros(n_taxes)
        # from the predicted array, get those elements that SHOULD BE tax
        real_pred = pred[real == tax]
        rp, rp_count = np.unique(real_pred, return_counts  = True)
        for idx, tax1 in enumerate(uniq_taxes):
            tax_count = np.append(rp_count[rp == tax1], 0)
            pred_counts[idx] += tax_count[0]
        confusion.append(pred_counts)
    # return taxon index as well as the confusion matrix
    return np.array(confusion), uniq_taxes
    
def build_cal_tab(pred_tax, real_tax, n_ranks=6):
    # build the calibration table from the given results
    # n_ranks 6 by default (phylum, class, order, family, genus, species), could be modifyed to include less/more (should be done automatically)
    
    results = []
    # remove first(query name) and last (K) columns from predicted taxons
    pred_cropped = pred_tax[:,1:-1].T
    real_mat = real_tax.to_numpy().T
    ranks = real_tax.columns
    
    for rank, pred, real in zip(ranks, pred_cropped, real_mat):
        # build a confusion matrix and get results from there, update results table
        rank_confusion, taxons = build_confusion(pred, real)
        rank_metrics = get_metrics(rank_confusion, taxons)
        rank_metrics = np.insert(rank_metrics, 0, rank, axis=1)
        results.append(rank_metrics)
    return np.concatenate(results)

    for rank in np.arange(n_ranks):
        # build a confusion matrix and get results from there, update results table
        rank_confusion, taxons = build_confusion(pred_cropped[:,rank], real_tax.iloc[:,rank].to_numpy())
        rank_metrics = get_metrics(rank_confusion, taxons)
        rank_metrics = np.insert(rank_metrics, 0, rank, axis=1)
        results.append(rank_metrics)
    return np.concatenate(results)

# TODO: loo_calibrate was moved to Calibrator0, delete this one
def loo_calibrate(garrus, w_size, w_step, max_k, step_k, max_sites, step_sites, dist_mat):
    # set calibration parameters & initialize results table    
    # set coordinate ranges for the sliding windows
    max_pos = garrus.loader.dims[1]
    start_range = np.arange(0, max_pos - w_size, w_step)
    if start_range[-1] < max_pos - w_size:
        # add a tail window, if needed, to cover the entire sequence
        np.append(start_range, max_pos - w_size)
    end_range = start_range + w_size
    w_coords = np.array([start_range, end_range]).T
    
    # k & sites ranges
    k_range = np.arange(1, max_k, step_k)
    site_range = np.arange(5, max_sites, step_sites)
    
    results = []
    
    # begin calibration
    for idx, (start, end) in enumerate(w_coords):
        print(f'Window {start} - {end} ({idx + 1} of {len(w_coords)})')
        # extract window and select atributes
        window = garrus.loader.get_window(start, end, garrus.row_thresh, garrus.col_thresh)
        if len(window.cons_mat) == 0:
            continue
        selector = fsele.Selector(window.cons_mat, window.cons_tax)
        selector.select_taxons(minseqs = garrus.min_seqs)
        selector.generate_diff_tab()
        
        # preprocess
        for n_sites in site_range:
            print(f'\t{n_sites} sites')
            # post collapse, generates the final data matrix and taxonomy table
            x,y, super_c = preproc.preprocess(selector, garrus.row_thresh, garrus.col_thresh, minseqs=10, nsites=n_sites)
            
            # loo_result tables are plain lists, later to be converted into np.arrays
            # these store the classifications for each query
            loo_maj = []
            loo_wknn = []
            loo_dwknn = []
            
            # iterate trought the data
            idx_gen = loo_generator(x.shape[0])
            n_seqs = x.shape[0]
            quintile = int(n_seqs / 5)
            
            # outer loop
            for train_idx, test_idx in idx_gen:
                if len(train_idx) == 0:
                    # no training sequences (only 1 sequence passed the filters)
                    continue
                if test_idx % quintile == 0:
                    print(f'\t\tCalibrated {test_idx} of {n_seqs} ({(test_idx/n_seqs) * 100:.2f}%)')
                
                # build the datasets
                query = x[test_idx]
                train_data = x[train_idx]
                train_tax = y.iloc[train_idx]
                
                # get classifications using each method
                results_maj, results_wknn, results_dwknn = classification.calibration_classify(query, k_range, train_data, train_tax, dist_mat, q_name=test_idx)
                
                # update classification tables
                for res_tab, loo_tab, mode in [(results_maj, loo_maj, 'majority'),
                                                 (results_wknn, loo_wknn, 'weighted'),
                                                 (results_dwknn, loo_dwknn, 'weighted')]:
                    for k, tab in zip(k_range, res_tab):
                        classif = classification.get_classif(tab, mode)
                        classif = np.insert(classif, [0, len(classif)], [test_idx, k])
                        loo_tab.append(classif)
            
            if len(loo_maj) == 0:
                continue
            
            # convert classification tables into np.arrays
            loo_maj = np.array(loo_maj)
            loo_wknn = np.array(loo_wknn)
            loo_dwknn = np.array(loo_dwknn)
            
            # get classification metrics
            cal_res = []
            # get metrics for each mode,K,n_sites combination
            for mode, loo_tab in enumerate((loo_maj, loo_wknn, loo_dwknn)):
                for k in k_range:
                    # select the results submatrix for the current K and get the metrics for each rank
                    k_submat = loo_tab[loo_tab[:,-1] == k]
                    cal_tab = build_cal_tab(k_submat, y)
                    # add remaining columns
                    cal_tab = np.insert(cal_tab, 2, start, axis=1)
                    cal_tab = np.insert(cal_tab, 3, end, axis=1)
                    cal_tab = np.insert(cal_tab, 4, k, axis=1)
                    cal_tab = np.insert(cal_tab, 5, n_sites, axis=1)
                    cal_tab = np.append(cal_tab, np.ones((len(cal_tab), 1))*mode, axis=1)
                    cal_res.append(cal_tab)
            
            # update results
            cal_res = np.concatenate(cal_res)
            results.append(cal_res)
    
    # build results table
    calibration_result = pd.DataFrame(np.concatenate(results),
                                      columns=['rank',
                                               'taxon',
                                               'w_start',
                                               'w_end',
                                               'K',
                                               'n_sites',
                                               'accuracy',
                                               'precision',
                                               'recall',
                                               'F1_score',
                                               'mode']) # do all three modes in a single table
    # translate numeric codes
    calibration_result['rank'].replace({0:'phylum',
                                        1:'class',
                                        2:'order',
                                        3:'family',
                                        4:'genus',
                                        5:'species'},
                                       inplace=True)
    calibration_result['mode'].replace({0:'maj',
                                        1:'wknn',
                                        2:'dwknn'},
                                       inplace=True)
    return calibration_result
#%% classes
class Calibrator:
    def __init__(self, out_dir, warn_dir):
        self.out_dir = out_dir
        self.warn_dir = warn_dir
        
        self.selector = fsele.Selector(out_dir)
        self.loader = None
        self.set_row_thresh()
        self.set_col_thresh()
        self.set_min_seqs()
        self.set_rank()
        self.set_cost_mat()
        self.report = None
    
    def set_database(self, mat_file, acc_file, tax_file, order_file):
        self.loader = windows.WindowLoader('Graboid.calibrator.windowloader')
        self.loader.set_files(mat_file, acc_file, tax_file)
        self.selector.load_order_mat(order_file)
    
    def set_row_thresh(self, thresh=0.2):
        self.row_thresh = thresh
        
    def set_col_thresh(self, thresh=0.2):
        self.col_thresh = thresh
    
    def set_min_seqs(self, min_seqs=10):
        self.min_seqs = min_seqs
    
    def set_rank(self, rank='genus'):
        self.rank = rank
    
    def set_dist_mat(self, mat_code):
        matrix = cost_matrix.get_matrix(mat_code)
        if matrix is None:
            print('Could not set distance matrix, invalid matrix code')
            return
        self.cost_mat = matrix
    
    def check_ready(self):
        # check that the calibration is ready to go
        missing = []
        try:
            self.cost_mat
        except AttributeError:
            missing.append('distance matrix')
        try:
            self.loader.matrix
            self.loader.bounds
            self.loader.dims
        except AttributeError:
            missing.append('alignment matrix')
        try:
            self.loader.accdist
        except AttributeError:
            missing.append('accession list')
        try:
            self.loader.tax_tab            
        except AttributeError:
            missing.append('taxonomy table')
        try:
            self.order_file
        except AttributeError:
            missing.append('order file')
        ready = len(missing) > 0
        return ready, missing
    
    def grid_search(self, w_size, w_step, max_k, step_k, max_n, step_n, min_k=1, min_n=5):
        if self.loader is None:
            return
        # set calibration parameters
        # set coordinate ranges for the sliding window
        max_pos = self.loader.dims[1]
        start_range = np.arange(0, max_pos - w_size, w_step)
        if start_range[-1] < max_pos - w_size:
            # add a tail window, if needed, to cover the entire sequence
            np.append(start_range, max_pos - w_size)
        end_range = start_range + w_size
        w_coords = np.array([start_range, end_range]).T
        
        # k & n ranges
        k_range = np.arange(min_k, max_k, step_k)
        n_range = np.arange(min_n, max_n, step_n)
        
        # begin calibration
        results = []
        
        for idx, (start, end) in enumerate(w_coords):
            print(f'Window {start} - {end} ({idx + 1} of {len(w_coords)})')
            # extract window and select atributes
            window = self.loader.get_window(start, end, self.row_thresh, self.col_thresh)
            if len(window.eff_mat) == 0:
                continue
            n_seqs = window.eff_mat.shape[0]
            quintile = int(n_seqs / 5)

            if n_seqs < self.min_seqs:
                # not enough sequences passed the filter, skip iteration
                print(f'Window {start} - {end}. Not enoug sequences to perform calibration ({n_seqs}, min = {self.min_seqs}), skipping')
                continue
            
            n_sites = self.selector.get_sites(n_range, start, end, self.rank)
            y = window.eff_tax
            prev_distances = np.zeros(window.shape[0], window.shape[0]-1, dtype = np.int8)
            reports1 = []
            for n, sites in n_sites.items():
                print(f'\t{n} sites')
                # post collapse, generates the final data matrix and taxonomy table
                x = window.eff_mat[:, sites - start]
                
                # loo_result tables are plain lists, later to be converted into np.arrays
                # these store the classifications for each query
                
                # iterate trought the data
                idx_gen = loo_generator(x.shape[0])
                
                reports0 = []
                # outer loop
                for train_idx, test_idx in idx_gen:
                    if test_idx % quintile == 0:
                        print(f'\t\tCalibrated {test_idx} of {n_seqs} ({(test_idx/n_seqs) * 100:.2f}%)')
                    
                    # build the datasets
                    query = x[test_idx]
                    train_data = x[train_idx]
                    train_tax = y.iloc[train_idx]
                    query_dists = prev_distances[test_idx]
                    
                    q_report, new_dists = classification.classify(query, train_data, self.dist_mat, train_tax, k_range, modes = 'mwd', prev_dists = query_dists, get_winners = True)
                    # q_report columns: idx, rk, tax, count, _k, average_dists, std_dists, total_support, median_support
                    prev_distances[test_idx] += new_dists
                    reports0.append(q_report)
                reports0 = pd.concat(reports0)
                reports0 = reports0.insert(2, 'w_start', start)
                reports0 = reports0.insert(3, 'w_end', end)
                reports0 = reports0.insert(4, 'n_sites', n)
                reports1.append(reports0)
                
            report = pd.concat(reports1)
            report.sort_values(['w_start', 'n_sites', '_k'], inplace=True, ignore_index=True)
            
            # get metrics for each tank/tax in each mode,K,n_sites combination
            for (w_start, n, k, mode), subtab0 in report.groupby(['w_start', 'n_sites', '_k', 'mode']):
                w_end = subtab0.w_end.iloc[0]
                for (rk, tax), subtab1 in subtab0.groupby(['rk', 'tax']):
                    pred = subtab1.tax.values
                    real = y.loc[pred.idx, rk].values
                    confusion, taxons = build_confusion(pred, real)
                    metrics = get_metrics(confusion, taxons)
                    metrics_report = pd.DataFrame(metrics, columns=['Accuracy', 'Precision', 'Recall', 'F1_score'])
                    metrics_report['w_start'] = w_start
                    metrics_report['w_end'] = w_end
                    metrics_report['rank'] = rk
                    metrics_report['taxon'] = tax
                    metrics_report['n_sites'] = n
                    metrics_report['K'] = k
                    metrics_report['mode'] = mode
                    
                    results.append(metrics_report)
        
        results = pd.concatenate(results)

        # translate numeric codes
        numcode_dict = {idx:rank for idx, rank in enumerate(self.loader.tax_tab.columns)}
        results['rank'].replace(numcode_dict, inplace=True)
        self.report = results
        self.meta = {'k':k_range,
                     'n':n_range,
                     'w_size':w_size,
                     'w_step':w_step}
    
    def save_report(self, filename=None):
        if filename is None:
            filename = time.strftime("report_%d%m%Y-%H%M%S")
        self.out_file = f'{self.out_dir}/{filename}.csv'
        self.meta_file = f'{self.out_dir}/{filename}.meta'
        if not self.report is None:
            self.report.to_csv(self.out_file)
            with open(self.meta_file, 'wb') as meta_handle:
                pickle.dump(self.meta, meta_handle)
