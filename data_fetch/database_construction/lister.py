#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Sep 20 10:33:36 2021

@author: hernan
This script generates accession list tables from the summary files generated by db_survey
"""

#%% libraries
from glob import glob
import pandas as pd
import os

#%% functions
# data loading
# using these readfuncs makes it easier to incorporate new databases in the future
def read_BOLD_summ(summ_file):
    # extract a list of accessions from a BOLD summary
    bold_tab = pd.read_csv(summ_file, sep = '\t', encoding = 'latin-1', dtype = str) # latin-1 to parse BOLD files
    accs = bold_tab['sampleid'].tolist()
    return accs

def read_NCBI_summ(summ_file):
    # extract a list of accessions from an NCBI or ENA summary
    ncbi_tab = pd.read_csv(summ_file, sep = '\t')
    accs = ncbi_tab.iloc[:,0].tolist()
    return accs

#%% classes
class SummProcessor():
    def __init__(self, taxon, marker, database, in_file, out_dir):
        self.taxon = taxon
        self.marker = marker
        self.database = database
        self.in_file = in_file
        self.out_dir = out_dir
        self.set_readfunc(database)
        self.build_acc_subtab()
        self.warnings = {0:[], 1:[]}
    
    def set_readfunc(self, dbase):
        # database determines how the file is read
        if dbase == 'BOLD':
            self.readfunc = read_BOLD_summ
        elif dbase == 'NCBI':
            self.readfunc = read_NCBI_summ
                
    def get_shortaccs_ver(self):
        # split the accession code from the version number, return both
        accs = self.readfunc(self.in_file)
        splitaccs = [acc.split('.') for acc in accs]
        vers = [acc[-1] if len(acc) > 1 else '1' for acc in splitaccs]
        shortaccs = [acc.split(f'.{ver}')[0] for acc, ver in zip(accs, vers)]
        return shortaccs, vers
    
    def build_acc_subtab(self):
        #  build table with index = short accession (no version number) and columns = accession, version number
        shortaccs, vers = self.get_shortaccs_ver()
        acc_subtab = pd.DataFrame({'Accession': shortaccs, 'Version':vers}, index = shortaccs)
        acc_subtab['Database'] = self.database
        acc_subtab['Status'] = 'New'
        self.acc_subtab = acc_subtab
        
        if len(self.acc_subtab) == 0:
            self.warnings[0].append(f'WARNING: No records in summary of the {self.database} database')

    def compare_with_old(self, old_tab):
        # see if any of the surveyed records already exist, if they do, see if there is a newer version
        intersect = self.acc_subtab.index(old_tab.index)
        compare = self.acc_subtab.loc[intersect, 'Version'] > self.old_tab.loc[intersect, 'Version']
        to_drop = compare.loc[~compare].index # drop accs if the version is <= than the old record
        to_replace = compare.loc[compare].index # replace records in old record if this version is greater
        
        self.acc_subtab.drop(to_drop, inplace = True)
        self.acc_subtab.at[to_replace, 'Status'] = 'Update'

        if len(to_drop) > 0 and len(self.acc_subtab) == 0:
            # all records dropped
            self.warnings[1].append(f'NOTICE: No new or updated records found in the {self.database} database')
    
class Lister():
    def __init__(self, taxon, marker, in_dir, warn_dir, old_file = None):
        self.taxon = taxon
        self.marker = marker
        self.in_dir = in_dir # TODO: agregar out_dir
        self.warn_dir = warn_dir
        self.old_file = old_file
        self.warnings = []
        self.__get_old_tab()
        self.out_file = f'{in_dir}/{taxon}_{marker}.acc' # TODO: cambiar in_dir por out_dir

    def __check_summaries(self):
        summ_files = glob(f'{self.in_dir}/*summ')
        summ_dict = {}
        for file in summ_files:
            db = file.split('_')[-1].split('.')[0]
            summ_dict[db] = file
        self.summ_dict = summ_dict

        if len(summ_dict) == 0:
            self.warnings.append(f'WARNING: No summaries found in dir {self.in_dir}')
    
    def __read_summs(self):
        processors = {}
        for db, file in self.summ_dict.items():
            processors[db] = SummProcessor(self.taxon, self.marker, db, file, self.in_dir)
        self.processors = processors
    
    def __merge_summs(self):
        # see if there are shared record between summaries, delete repetitions, prioritizing NCBI summaries
        if len(self.processors) <= 1:
            return
        merged = pd.concat([proc.acc_subtab for proc in self.processors.values()])
        repeated = merged['Accession'].value_counts() > 1
        rep_idx = repeated.loc[repeated].index
        sub_merged = merged.loc[rep_idx]
        
        for acc, subtab in sub_merged.groupby('Accession'):
            lead = 'NCBI'
            if not 'NCBI' in subtab['Database'].values:
                lead = subtab['Database'].values[0]
            # drop accession from all database summaries except lead's
            to_drop = subtab.loc[subtab['Database'] != lead, 'Database'].values
            for db in to_drop:
                self.processors[db].acc_subtab.drop(acc, inplace = True)
    
    def __get_old_tab(self):
        # retrieve accession table from previous run (use it to update)
        if self.old_file is None:
            self.old_tab = None
        else:
            self.old_tab = pd.read_csv(self.old_file) # TODO make sure this works properly (ep, index col)
    
    def __compare_with_old(self):
        # check which records are new or updated
        if self.old_tab is None:
            return
        for proc in self.processors.values():
            proc.compare_with_old(self.old_tab)

    def process(self):
        self.__check_summaries()
        self.__read_summs()
        self.__merge_summs()
        self.__compare_with_old()
    
    def store_list(self):
        # store filtered records in a single file (columns: Accession, Version, Database, Status), status indicates wether a record is new or being updated
        merged = pd.concat([proc.acc_subtab for proc in self.processors.values()])
        merged.to_csv(self.out_file)
    
    def check_warnings(self):
        # collect warnings from summary processors and generate warning file
        w0 = []
        w1 = []
        for proc in self.processors.values():
            w0 += proc.warnings[0]
            w1 += proc.warnings[1]
        self.warnings += w0 + w1
        
        if len(self.warnings) > 0:
            with open(f'{self.warn_dir}/warnings.lstr', 'w') as handle:
                handle.write('\n'.join(self.warnings))
    
    def make_list(self):
        self.process()
        self.store_list()
        self.check_warnings()

class PreLister():
    # This class is used within the Surveyor class to compare the summary with the previous databse (if present)
    def __init__(self, taxon, marker, in_file, out_dir, warn_dir, old_file = None):
        self.taxon = taxon
        self.marker = marker
        self.in_file = in_file
        self.out_dir = out_dir
        self.warn_dir = warn_dir
        self.old_file = old_file
        self.warnings = ['']
        self.out_file = f'{out_dir}/{taxon}_{marker}_NCBI.acc'
    
    def __read_infile(self):
        if not os.path.isfile(self.in_file):
            self.warnings.append(f'WARNING: file {self.in_file} not found')
            return
        
        acc_dict = {}
        with open(self.in_file, 'r') as handle:
            for line in handle.read().splitlines():
                split_line = line.split('.')
                shortacc = split_line[0]
                version = split_line[1]
                acc_dict[shortacc] = (line, version)
        
        if len(acc_dict) == 0:
            self.warnings.append(f'WARNING: file {self.in_file} is empty')
            self.acc_tab = None
            return

        self.acc_tab = pd.DataFrame.from_dict(acc_dict, orient = 'index', columns = ['Accession', 'Version'])
    
    def __read_oldfile(self):
        if self.old_file is None:
            self.old_tab = None
            return
        
        if not os.path.isfile(self.old_file):
            self.warnings.append(f'WARNING: file {self.in_file} not found')
            return

        if len(self.old_tab) == 0:
            self.warnings.append(f'WARNING: file {self.old_file} is empty')
            self.old_tab = None
            return

        self.old_tab = pd.read_csv(self.old_file, index_col = 0)
        
    
    def merge_with_old(self):
        if not self.acc_tab is None:
            if not self.old_tab is None:
                intersect = self.acc_tab.index.intersection(self.old_tab.index)
                sub_acc = self.acc_tab.loc[intersect]
                old_acc = self.old_tab.loc[intersect]

                filtered = sub_acc.loc[old_acc['Version'] >= sub_acc['Version']].index # records to drop
                self.out_tab = self.acc_tab.drop(index = filtered)
                return
            
            self.out_tab = self.acc_tab
            return
        
        self.out_tab = None
        return
    
    def save_tab(self):
        if self.out_tab is None:
            self.warnings.append('WARNING: No out file generated')
            return
        if len(self.out_tab) == 0:
            self.warnings.append(f'WARNING: No new sequences in {self.in_file}')
            return
        
        self.out_tab.to_csv(self.out_file)
    
    def save_warnings(self):
        if len(self.warnings) > 1:
            with open(f'{self.warn_dir}/warnings.surv', 'a') as warn_handle:
                warn_handle.write('\n'.join(self.warnings))
    
    def pre_list(self):
        self.__read_infile()
        self.__read_oldfile()
        self.merge_with_old()
        self.save_tab()
        self.save_warnings()