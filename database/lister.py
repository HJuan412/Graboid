#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Sep 20 10:33:36 2021

@author: hernan
This script generates accession list tables from the summary files generated by db_survey
"""

#%% libraries
import logging
import pandas as pd
import re

#%% set logger
logger = logging.getLogger('Graboid.database.lister')

#%% functions
# data loading
# using these readfuncs makes it easier to incorporate new databases in the future
def read_BOLD_summ(summ_file):
    # extract a list of accessions from a BOLD summary
    bold_tab = pd.read_csv(summ_file, sep = '\t', encoding = 'latin-1', dtype = str) # latin-1 to parse BOLD files
    # sometimes BOLD has multiple records, remove repeats
    duplicated = bold_tab.loc[bold_tab.sampleid.duplicated()]
    single = bold_tab.loc[~bold_tab.sampleid.duplicated()].reset_index(drop=False).set_index('sampleid', drop=False)
    for samp_id, cluster in duplicated.groupby('sampleid'):
        seqs = cluster.nucleotides.sum()
        single.loc[samp_id, 'nucleotides'] += seqs
    accs = single.sampleid.tolist()
    single.to_csv(summ_file, sep='\t', index=False)
    return accs

def read_NCBI_summ(summ_file):
    # extract a list of accessions from an NCBI or ENA summary
    ncbi_tab = pd.read_csv(summ_file, sep = '\t')
    accs = ncbi_tab.iloc[:,0].tolist()
    return accs

def read_summ(summ_file, database):
    readers = {'NCBI':read_NCBI_summ,
               'BOLD':read_BOLD_summ}
    accs = readers[database](summ_file)
    # if database == 'NCBI':
    #     ncbi_tab = pd.read_csv(summ_file, sep = '\t')
    #     accs = ncbi_tab.iloc[:,0].tolist()
    # elif database == 'BOLD':
    #     bold_tab = pd.read_csv(summ_file, sep = '\t', encoding = 'latin-1', dtype = str) # latin-1 to parse BOLD files
    #     accs = bold_tab['sampleid'].tolist()
    
    if len(accs) == 0:
        raise Exception(f'No records present in {summ_file}')
    return accs

# summary manipulation
def get_shortaccs_ver(acc_list):
    # split the accession code from the version number, return both
    # assign version number 1 whenever necesary
    splitaccs = [acc.split('.') for acc in acc_list]
    vers = [acc[-1] if len(acc) > 1 else '1' for acc in splitaccs]
    shortaccs = [acc.split(f'.{ver}')[0] for acc, ver in zip(acc_list, vers)]
    return shortaccs, vers

def build_acc_subtab(acc_list, database):
    # build table with index = short accession (no version number) and columns = accession, version number
    shortaccs, vers = get_shortaccs_ver(acc_list)
    acc_subtab = pd.DataFrame({'Accession': shortaccs, 'Version':vers}, index = shortaccs)
    acc_subtab['Database'] = database
    return acc_subtab

def clear_repeats(merged_tab):
    # locates and clears repeated records in the merged table, prioritizing NCBI records when possible
    # locate repeated indexes
    rep_idx = merged_tab.index[merged_tab.index.duplicated()]
    nbases = merged_tab['Database'].nunique()
    if len(rep_idx) == 0:
        # no repeats, continue with original table
        return merged_tab
    
    logger.info(f'Found {len(rep_idx)} repeated records between {nbases} databases')
    
    # handle repeats
    to_keep = []
    for idx in rep_idx:
        # filter repeated entries for the same record
        sub_merged = merged_tab.loc[idx]
        # try to use an NCBI record if available, otherwise pick a substitute
        lead = 'NCBI'
        if not 'NCBI' in sub_merged['Database'].values:
            lead = sub_merged['Database'].values[0]
        to_keep = sub_merged.loc[sub_merged['Database'] == lead].iloc[[0]] # double bracket lets iloc extract a single row as a dataframe
    
    # generate final table
    cropped_merged = merged_tab.drop(index = rep_idx)
    clear_merged = pd.concat([cropped_merged, pd.DataFrame(to_keep)])
    
    logger.info(f'Listed {len(clear_merged)} records between {nbases} databases')
    return clear_merged

#%% classes
class Lister:
    # this class compares summaries of multiple database and generates a consensus
    # prioritizes NCBI in case of conflict
    valid_databases = ['BOLD', 'NCBI']
    def __init__(self, out_dir):
        self.summ_files = {}
        self.out_file = None
        self.out_dir = out_dir
    
    def get_summ_files(self, summ_files):
        self.summ_files = {}
        for db, file in summ_files.items():
            if db in self.valid_databases:
                self.summ_files.update({db:file})
                sample_file = file
            else:
                logger.warning(f'Database {db} not valid. File {file} will be ignored')
        if len(self.summ_files) == 0:
            raise Exception('No valid survey files detected')
        # generate the output file
        self.out_file = re.sub('.*/', self.out_dir + '/', re.sub('__.*', '.acc', sample_file))
    
    def build_list(self, summ_files):
        # summ_files dict with {database:summ_file}
        # generates a consensus list of accession codes from the summary files
        # detect summary files
        try:
            self.get_summ_files(summ_files)
        except:
            raise
        
        # read summ_files
        acc_tabs = []
        for db, file in summ_files.items():
            try:
                db_accs = read_summ(file, db)
            except Exception as excp:
                logger.warning(excp)
                continue
            acc_subtab = build_acc_subtab(db_accs, db)
            acc_tabs.append(acc_subtab)
        
        if len(acc_tabs) == 0:
            raise Exception('No records located. Repeat survey step')
        # merge subtabs
        merged = pd.concat(acc_tabs)
        merged_clear = clear_repeats(merged)
        
        # store tab
        merged_clear.to_csv(self.out_file)
        logger.info(f'Accession list stored to {self.out_file}')
